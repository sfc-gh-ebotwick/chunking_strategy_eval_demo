{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "lastEditStatus": {
   "notebookId": "tts46q62kjpyi7capk6k",
   "authorId": "5095547476787",
   "authorName": "EBOTWICK",
   "authorEmail": "elliott.botwick@snowflake.com",
   "sessionId": "f2f0793c-2cd5-439c-823c-03888b885a9d",
   "lastEditTime": 1756833990868
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Libraries",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "import snowflake.snowpark as snowpark\n",
    "\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "code",
   "id": "817c3622-da65-4ee3-9f1f-da15e94ec8d7",
   "metadata": {
    "language": "python",
    "name": "define_params",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "DB_NAME = \"CHUNKING_EVAL_DEMO\"\nSCHEMA_NAME = \"DATA\"\nSTAGE_NAME = \"DOCS\"\nWH_NAME = \"CHUNKING_EVAL_WAREHOUSE\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64842e12-1e4c-423b-a568-376102123485",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "list_files_in_stage",
    "resultHeight": 426
   },
   "outputs": [],
   "source": "-- List files in the stage to identify PDFs\nLS @{{DB_NAME}}.{{SCHEMA_NAME}}.{{STAGE_NAME}}"
  },
  {
   "cell_type": "code",
   "id": "4533b750-4e92-441b-8c21-4ed0c71b7384",
   "metadata": {
    "language": "python",
    "name": "upload_pdfs_to_stage",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import os\n\n#Define quick function to add local file to snowflake stage\ndef upload_file_to_stage(filename: str, target_stage: str):\n    print(f\"Adding file {filename} to {target_stage} stage...\")\n    put_result =  session.file.put(\n        local_file_name = f\"pdfs/{filename}\",\n        stage_location =  f'@\"{target_stage}\"',\n        auto_compress = False,\n        source_compression= 'AUTO_DETECT',\n        overwrite = True)\n    return put_result\n\n\nlocal_pdfs = []\nfor root, dirs, files  in os.walk('pdfs/'):\n    for file in files:\n        if file.endswith('.pdf'):\n            local_pdfs.append(file)\n\n\n#Get list of filenames already in stage\nstage_pdfs = [row[0][row[0].find('/')+1:] for row in session.sql(f'LS @{DB_NAME}.{SCHEMA_NAME}.{STAGE_NAME}').select('\"name\"').collect()]\n\nfor pdf in local_pdfs:\n    if pdf in stage_pdfs:\n        print(f\"File {pdf} already in {STAGE_NAME} stage!\")\n    else:\n        upload_file_to_stage(pdf, STAGE_NAME)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d25663e4-08a0-418c-a6d4-96e913aef549",
   "metadata": {
    "collapsed": false,
    "name": "Step1"
   },
   "source": [
    "## Step 1: Parse and Chunk Text from PDFs\n",
    "We begin by parsing the content of uploaded PDFs and chunking the text using Snowflake's [PARSED_TEXT](https://docs.snowflake.com/sql-reference/functions/parse_document-snowflake-cortex) and [SPLIT_TEXT_RECURSIVE_CHARACTER](https://docs.snowflake.com/sql-reference/functions/split_text_recursive_character-snowflake-cortex) features. These steps structure the text into manageable segments optimized for retrieval. To ensure that the PDF parsing and chunking have been processed correctly, we run queries on the parsed and chunked tables. This step helps verify the integrity of the content.\n",
    "\n",
    "Objective: **Transform unstructured content into indexed chunks for efficient search and retrieval.**\n",
    "\n",
    "Key Outputs:\n",
    "- SKO.HOP.PARSED_TEXT: Table containing the raw text.\n",
    "- SKO.HOP.CORTEX_CHUNK: Chunked, searchable content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f293bf7-06b5-4d05-9666-ad3096d25a31",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "CreateParsedTextTable",
    "resultHeight": 111
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (579301450.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    -- Create a table to hold the extracted text from the PDF files loaded in the SKO.HOP.RAG stage\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": "-- Create a table to hold the extracted text from the PDF files\nCREATE OR REPLACE TABLE {{DB_NAME}}.{{SCHEMA_NAME}}.PARSED_TEXT (relative_path VARCHAR(500), raw_text VARIANT);"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e7c0b-6c8f-472f-9cfe-94e1351b9925",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "UseParseDocument",
    "resultHeight": 111
   },
   "outputs": [],
   "source": "INSERT INTO {{DB_NAME}}.{{SCHEMA_NAME}}.PARSED_TEXT (relative_path, raw_text)\nWITH pdf_files AS (\n    SELECT DISTINCT\n        METADATA$FILENAME AS relative_path\n    FROM @{{DB_NAME}}.{{SCHEMA_NAME}}.DOCS\n    WHERE METADATA$FILENAME ILIKE '%.pdf'\n      -- Exclude files that have already been parsed\n      AND METADATA$FILENAME NOT IN (SELECT relative_path FROM PARSED_TEXT)\n)\nSELECT \n    relative_path,\n    SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n        '@{{DB_NAME}}.{{SCHEMA_NAME}}.DOCS',  -- Your stage name\n        relative_path,  -- File path\n        {'mode': 'layout'}  -- Adjust mode as needed ('layout', 'ocr')\n    ) AS raw_text\nFROM pdf_files;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2fe17b-9580-43b4-8b23-fb61a695df6c",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "ViewParsedText"
   },
   "outputs": [],
   "source": "-- inspect the results and count the tokens for each document\nSELECT *, SNOWFLAKE.CORTEX.COUNT_TOKENS('mistral-7b', RAW_TEXT) as token_count\nFROM {{DB_NAME}}.{{SCHEMA_NAME}}.PARSED_TEXT;"
  },
  {
   "cell_type": "code",
   "id": "287dbe74-8a7e-4b7c-9651-ee562453d2ea",
   "metadata": {
    "language": "sql",
    "name": "summarize_docs"
   },
   "outputs": [],
   "source": "ALTER TABLE PARSED_TEXT \n    ADD COLUMN IF NOT EXISTS DOC_SUMMARY VARCHAR(5000);\n\n\nUPDATE PARSED_TEXT \n  SET DOC_SUMMARY = AI_COMPLETE('claude-4-sonnet', CONCAT('Concisely summarize the following text of meeting minutes', RAW_TEXT))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dce5839-f0eb-4d4f-838e-9b3633979c8c",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "TestingSplitTextFunction",
    "resultHeight": 111,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Chunk the text based on paragraph seperators and write into DOC_CHUNKS_TABLE;\n\nCREATE OR REPLACE TABLE {{DB_NAME}}.{{SCHEMA_NAME}}.PARAGRAPH_CHUNKS AS\nWITH text_chunks AS (\n    SELECT\n        relative_path,\n        doc_summary,\n        SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n            raw_text:content::STRING,  -- Extract the 'content' field from the JSON\n            'markdown', -- Adjust to 'markdown' if needed\n            2000,       -- Adjust chunk size\n            100,        -- Adjust overlap size\n            ['\\n\\n']    -- Adjust separators\n        ) AS chunks\n    FROM {{DB_NAME}}.{{SCHEMA_NAME}}.PARSED_TEXT\n)\nSELECT\n    relative_path,\n    doc_summary,\n    c.value AS chunk  -- Extract each chunk of the parsed text\nFROM text_chunks,\nLATERAL FLATTEN(INPUT => chunks) c;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64cfb61-0168-43ea-9a70-8242dc45ec07",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "ViewChunkedText",
    "resultHeight": 438,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Check the results and \n\nSELECT *, SNOWFLAKE.CORTEX.COUNT_TOKENS('mistral-7b', CHUNK) as token_count\nFROM {{DB_NAME}}.{{SCHEMA_NAME}}.PARAGRAPH_CHUNKS;"
  },
  {
   "cell_type": "code",
   "id": "d0958f27-5945-4e2d-becd-04b41d7e690f",
   "metadata": {
    "language": "sql",
    "name": "add_metadata",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "ALTER TABLE PARAGRAPH_CHUNKS ADD COLUMN IF NOT EXISTS MEETING_YEAR VARCHAR(4);\nALTER TABLE PARAGRAPH_CHUNKS ADD COLUMN IF NOT EXISTS MEETING_MONTH VARCHAR(2);\nALTER TABLE PARAGRAPH_CHUNKS ADD COLUMN IF NOT EXISTS ABSOLUTE_PATH VARCHAR(500);\n\n\nUPDATE PARAGRAPH_CHUNKS \n  SET MEETING_YEAR = SUBSTRING(RELATIVE_PATH, 14, 4),\n      MEETING_MONTH = SUBSTRING(RELATIVE_PATH, 19, 2),\n      ABSOLUTE_PATH =  GET_ABSOLUTE_PATH('@DOCS', CONCAT('docs/', RELATIVE_PATH));",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bd788c5a-b3b4-42e8-8369-a316fb8cf966",
   "metadata": {
    "language": "sql",
    "name": "update_chunk_dtype",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "ALTER TABLE PARAGRAPH_CHUNKS\n    ADD COLUMN TEMP_COL VARCHAR;\n\nUPDATE PARAGRAPH_CHUNKS\n    SET TEMP_COL = CAST(CHUNK AS VARCHAR);\n\nALTER TABLE PARAGRAPH_CHUNKS\n    DROP COLUMN CHUNK;\n    \nALTER TABLE PARAGRAPH_CHUNKS\n    RENAME COLUMN TEMP_COL TO CHUNK;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "022b49c4-6486-421b-b8e1-1c21a748419c",
   "metadata": {
    "language": "sql",
    "name": "update_chunks_with_doc_summaries"
   },
   "outputs": [],
   "source": "ALTER TABLE PARAGRAPH_CHUNKS ADD COLUMN IF NOT EXISTS CHUNK_WITH_SUMMARY VARCHAR;\n\n\nUPDATE PARAGRAPH_CHUNKS \n  SET CHUNK_WITH_SUMMARY = DOC_SUMMARY || '\\n\\n' || CHUNK",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9fff25d4-150c-4767-8717-428adc1eb105",
   "metadata": {
    "language": "sql",
    "name": "CreateCSS_RawChunks"
   },
   "outputs": [],
   "source": "-- Create a search service over your new chunked pdf table\n\nCREATE OR REPLACE CORTEX SEARCH SERVICE {{DB_NAME}}.{{SCHEMA_NAME}}.FOMC_RAW_TEXT_RETRIEVAL\n    ON SEARCH_COL\n    ATTRIBUTES MEETING_YEAR, MEETING_MONTH\n    WAREHOUSE = {{WH_NAME}}\n    TARGET_LAG = '1 hour'\n    AS SELECT \n        ABSOLUTE_PATH,\n        RELATIVE_PATH,\n        CHUNK,\n        MEETING_YEAR,\n        MEETING_MONTH,\n        CHUNK AS SEARCH_COL\n    FROM {{DB_NAME}}.{{SCHEMA_NAME}}.PARAGRAPH_CHUNKS;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a330f9-c774-47f2-b8ca-031ae441c602",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "CreateCSS_SummaryChunks",
    "resultHeight": 111
   },
   "outputs": [],
   "source": "-- Create a search service over your new chunked pdf table\n\nCREATE OR REPLACE CORTEX SEARCH SERVICE {{DB_NAME}}.{{SCHEMA_NAME}}.FOMC_TAGGED_CHUNK_RETRIEVAL\n    ON SEARCH_COL\n    ATTRIBUTES MEETING_YEAR, MEETING_MONTH\n    WAREHOUSE = {{WH_NAME}}\n    TARGET_LAG = '1 hour'\n    AS SELECT \n        ABSOLUTE_PATH,\n        RELATIVE_PATH,\n        CHUNK,\n        MEETING_YEAR,\n        MEETING_MONTH,\n        CHUNK_WITH_SUMMARY as SEARCH_COL\n    FROM {{DB_NAME}}.{{SCHEMA_NAME}}.PARAGRAPH_CHUNKS;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19284d0-107c-4c7f-aa2e-5378159c4a3b",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "CheckSearchResults"
   },
   "outputs": [],
   "source": "# Query your Snowflake Cortex Search Service using the Snowpark Python API to retrieve and process search results.\nimport streamlit as st\nfrom snowflake.snowpark import Session\nfrom snowflake.core import Root\nroot = Root(session)\n\nfomc_search_service = (root\n  .databases[DB_NAME]\n  .schemas[SCHEMA_NAME]\n  .cortex_search_services['FOMC_TAGGED_CHUNK_RETRIEVAL']\n)\n\nresp = fomc_search_service.search(\n  query=\"\"\"How has global economic economy shaped interest rates in early 2024\"\"\",\n  columns=['SEARCH_COL', 'RELATIVE_PATH', 'MEETING_YEAR'],\n  filter={\"@eq\": {\"MEETING_YEAR\": \"2024\"} },\n  limit=3\n)\nresults = resp.results\nresults\n\n# context_str = \"\"\n# for i, r in enumerate(results):\n#     context_str = f\"Document {i+1}: \\n\\n {r['RELATIVE_PATH']} \\n {r['CHUNK']}\\n****************\\n\"\n\n#     st.write(context_str)"
  },
  {
   "cell_type": "code",
   "id": "01d7637e-a338-43db-8673-dd400e34a688",
   "metadata": {
    "language": "python",
    "name": "test_rag"
   },
   "outputs": [],
   "source": "from snowflake.cortex import complete\nimport streamlit as st\n\nst.write(complete('claude-4-sonnet', \n         f\"\"\"Use the provided context to answer the user question. \n         Question: How has global economic economy shaped interest rates in early 2024? \n         Context :{results[0]['SEARCH_COL']}\"\"\"))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "49decb9a-7340-43bf-b7bd-2b779e954e19",
   "metadata": {
    "language": "sql",
    "name": "summarize_pdfs"
   },
   "outputs": [],
   "source": "SELECT *,  SNOWFLAKE.CORTEX.COMPLETE('CLAUDE-4-SONNET', CONCAT('SUMMARIZE THE FOLLOWING TEXT SUCCINTLY AND PROVIDE A FEW QUICK SAMPLE QUESTIONS A USER MAY ASK OF THIS DOCUMENT', RAW_TEXT)) FROM PARSED_TEXT",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34e586b9-b2a3-4415-b33e-48757949c573",
   "metadata": {
    "language": "sql",
    "name": "ARCHIVE_ai_summarize_agg"
   },
   "outputs": [],
   "source": "WITH chunks AS (\n    SELECT\n        t.\"RELATIVE_PATH\" AS FILE_NAME,\n        c.value::varchar AS chunk          -- plainâ€‘text chunk\n    FROM PARSED_TEXT AS t\n    ,   LATERAL FLATTEN(\n            input => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n                t.\"RAW_TEXT\",\n                'markdown',\n                512,       -- Adjust chunk size\n                64,        -- Adjust overlap size\n                ['\\n\\n']    -- Adjust separators\n            )\n        ) AS c\n),\n\n/* ----------------------------------------------------------------------\n   2. One summary per document\n------------------------------------------------------------------------ */\ndoc_summaries AS (\n    SELECT\n        FILE_NAME,\n        AI_SUMMARIZE_AGG(chunk) AS doc_summary\n    FROM chunks\n    GROUP BY FILE_NAME\n)\n\n/* ----------------------------------------------------------------------\n   3. Attach the summary and prepend it to each chunk\n------------------------------------------------------------------------ */\nSELECT\n    c.FILE_NAME,\n    d.doc_summary || '\\n\\n' || c.chunk AS chunk   -- summary + chunk\nFROM chunks        AS c\nJOIN doc_summaries AS d\n ON c.FILE_NAME = d.FILE_NAME;\n \n--  /* ---------- contextual chunk service ---------- */\n-- CREATE OR REPLACE CORTEX SEARCH SERVICE contextualized_chunk_search_svc\n--   ON chunk\n--   ATTRIBUTES (ID, FILE_NAME)\n--   WAREHOUSE   = COMPUTE\n--   TARGET_LAG  = '1 hour'\n--   EMBEDDING_MODEL = 'snowflake-arctic-embed-m-v1.5'\n--   AS (\n--       SELECT ID, FILE_NAME, chunk\n--       FROM   contextual_chunks\n--   );\n",
   "execution_count": null
  }
 ]
}